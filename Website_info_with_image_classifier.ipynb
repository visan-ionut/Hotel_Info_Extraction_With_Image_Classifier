{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This code automates the collection and classification of information and images from the official website of a hotel. Using DuckDuckGo to find the hotel's main page, it extracts essential data such as address, description, contact details, services, and GPS coordinates. The description is summarized using the T5 Transformer model, and all images on the site are downloaded and classified using a Convolutional Neural Network (CNN) model. The classified images are then organized and stored locally based on their categories.\n",
        "\n",
        "To meet the requirements is necessary to create an input file, an output file, and integrating the CNN model, as well as ensuring the output contains the specified categories (activities, conference, hotel, pool, restaurant, room, sightseeing, spa).\n",
        "\n",
        "The purpose of this project is purely educational, to understand the process of web scraping and image classification. It is very likely that the images may not be classified very accurately as the training capacity of the model has been limited in terms of data.\n",
        "\n",
        "For better accuracy, we encourage you to use the provided model and adjust its performance (CNN structure and training dataset).\n",
        "\n",
        "Credit: Poață Andrei Cătălin (UNSTPB, Artificial Intelligence master), Ionuț Vișan (UNSTPB, Artificial Intelligence master)\n",
        "\n",
        "!! We do not encourage the use of web scraping techniques that violate the terms and conditions of websites."
      ],
      "metadata": {
        "id": "I597YFEpyL7_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports"
      ],
      "metadata": {
        "id": "XRrpfS8uvIku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pycountry"
      ],
      "metadata": {
        "id": "KJ1loV5TvMM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "import torch\n",
        "import os\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import torch.nn.functional as functional\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import shutil\n",
        "import re\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "import pprint\n",
        "import requests\n",
        "from PIL import Image\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urlparse, urlunparse, urljoin\n",
        "from collections import Counter\n",
        "import time\n",
        "import pycountry"
      ],
      "metadata": {
        "id": "Ox31Upndu8JQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Constants"
      ],
      "metadata": {
        "id": "cktU0fj5vHDf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You need to complete:\n",
        "\n",
        "HOTEL_NAME\n",
        "\n",
        "CITY\n",
        "\n",
        "By entering this data, you will be able to search for the hotel's website and receive all the information mentioned below."
      ],
      "metadata": {
        "id": "RbqaiKkb0cHo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_PATHS = [\"data/activities\", \"data/conference\", \"data/hotel\", \"data/pool\", \"data/restaurant\", \"data/room\", \"data/sightseeing\", \"data/spa\"]\n",
        "MODEL_PATH = './cnn.pth'\n",
        "INPUT_PATH = \"input\"\n",
        "OUTPUT_PATH = \"output\"\n",
        "HOTEL_NAME = \"xxxx\"\n",
        "CITY = \"xxxx\"\n",
        "num_to_label = {0: 'activities', 1: 'conference', 2: 'hotel', 3: 'pool', 4: 'restaurant', 5: 'room', 6: 'sightseeing', 7: 'spa'}\n",
        "label_to_num = {'activities': 0, 'conference': 1, 'hotel': 2, 'pool': 3, 'restaurant': 4, 'room': 5, 'sightseeing': 6, 'spa': 7}"
      ],
      "metadata": {
        "id": "xJVuBt4iu6z3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Returns:\n",
        "- Homepage\n",
        "- Address\n",
        "- Description\n",
        "- Email\n",
        "- Latitude\n",
        "- Longitude\n",
        "- Phone\n",
        "- Services\n",
        "- Summary (using T5 Transformer)\n",
        "- All the links inside the page\n",
        "- All the images inside the page\n",
        "- Images classified using CNN model"
      ],
      "metadata": {
        "id": "dCe1AT2RvNbH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_iDyfTEutKG"
      },
      "outputs": [],
      "source": [
        "def get_hotel_website_duckduckgo(hotel_name, city):\n",
        "    # Construct the search query\n",
        "    query = f\"{hotel_name} {city} site oficial\"\n",
        "    # Construct the DuckDuckGo search URL\n",
        "    url = f\"https://duckduckgo.com/html/?q={query}\"\n",
        "    # Define the headers for the HTTP request\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
        "    }\n",
        "    # Send the HTTP GET request to DuckDuckGo\n",
        "    response = requests.get(url, headers=headers)\n",
        "    if response.status_code == 200:\n",
        "        # Parse the HTML content of the response\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        # Define a list of domains to exclude from the search results\n",
        "        exclude_domains = [\n",
        "            \"booking.com\", \"tripadvisor.com\", \"expedia.com\", \"hotels.com\",\n",
        "            \"agoda.com\", \"airbnb.com\", \"guestreservations.com\", \"travelocity.com\",\n",
        "            \"orbitz.com\", \"priceline.com\", \"kayak.com\", \"reservations.com\", \"facebook.com\",\n",
        "            \"reservationstays.com\", \"instagram.com\", \"trivago.com\"\n",
        "        ]\n",
        "        # Iterate over all <a> tags with href attributes\n",
        "        for a in soup.find_all('a', href=True):\n",
        "            href = a['href']\n",
        "            if \"http\" in href and \"duckduckgo.com\" not in href and \"translate.duckduckgo\" not in href:\n",
        "                # Check if the URL does not belong to any excluded domain\n",
        "                if not any(domain in href for domain in exclude_domains):\n",
        "                    # Check if the hotel name is part of the URL\n",
        "                    if any(hotel_name.lower() in href.lower() for hotel_name in hotel_name.split()):\n",
        "                        # Parse and reconstruct the URL to its main page\n",
        "                        parsed_url = urlparse(href)\n",
        "                        main_page_url = urlunparse((parsed_url.scheme, parsed_url.netloc, '', '', '', ''))\n",
        "                        return main_page_url\n",
        "    else:\n",
        "        # Print error message if the search request failed\n",
        "        print(f\"Failed to retrieve search results: {response.status_code}\")\n",
        "        return None\n",
        "\n",
        "def get_most_frequent_website(hotel_name, city, attempts=3):\n",
        "    # Start timing the function execution\n",
        "    start_time = time.time()\n",
        "    results = []\n",
        "    # Attempt to retrieve the hotel website multiple times\n",
        "    for _ in range(attempts):\n",
        "        website = get_hotel_website_duckduckgo(hotel_name, city)\n",
        "        if website:\n",
        "            results.append(website)\n",
        "        # Add a delay between attempts to avoid rate limiting\n",
        "        time.sleep(2)\n",
        "\n",
        "    # Count the frequency of each retrieved website\n",
        "    website_counts = Counter(results)\n",
        "    if website_counts:\n",
        "        most_common_website = website_counts.most_common(1)[0][0]\n",
        "    else:\n",
        "        most_common_website = None\n",
        "\n",
        "    # End timing the function execution\n",
        "    end_time = time.time()\n",
        "    execution_time = end_time - start_time\n",
        "\n",
        "    # Print the results and execution time\n",
        "    print(\"Results obtained in 3 attempts:\")\n",
        "    for result in results:\n",
        "        print(result)\n",
        "\n",
        "    print(f\"Most frequent website for {hotel_name} in {city}: {most_common_website}\")\n",
        "    print(f\"Total execution time: {execution_time:.2f} seconds\")\n",
        "\n",
        "    return most_common_website\n",
        "\n",
        "def fetch_hotel_info(url):\n",
        "    # Define the headers for the HTTP request\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
        "    }\n",
        "    # Send the HTTP GET request to the hotel website\n",
        "    response = requests.get(url, headers=headers)\n",
        "    if response.status_code == 200:\n",
        "        response.encoding = 'utf-8'\n",
        "        # Parse the HTML content of the response\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Extract the hotel description\n",
        "        description = None\n",
        "        description_p = soup.find('p', class_='text')\n",
        "        if description_p:\n",
        "            description = description_p.get_text(separator=\"\\n\", strip=True)\n",
        "        else:\n",
        "            description_meta = soup.find('meta', attrs={'name': 'description'})\n",
        "            if description_meta:\n",
        "                description = description_meta.get('content', '').strip()\n",
        "\n",
        "        # Extract latitude and longitude\n",
        "        latitude = longitude = None\n",
        "        script_tag = soup.find('script', string=re.compile(r'var hotel_gps_coordinates'))\n",
        "        if script_tag:\n",
        "            script_content = script_tag.string\n",
        "            latitude = re.search(r'latitude\\s*:\\s*([\\d.]+)', script_content).group(1)\n",
        "            longitude = re.search(r'longitude\\s*:\\s*([\\d.]+)', script_content).group(1)\n",
        "\n",
        "        # Extract address\n",
        "        address = None\n",
        "        address_span = soup.find('span', class_='element element_address')\n",
        "        if address_span:\n",
        "            address = address_span.get_text(separator=\"\\n\", strip=True)\n",
        "\n",
        "        # Extract contact information (phone and email)\n",
        "        phone = email = None\n",
        "        phone_tag = soup.find('span', class_='phone element')\n",
        "        if phone_tag:\n",
        "            phone = phone_tag.find_next('a').get('href').replace('tel:', '')\n",
        "        email_tag = soup.find('span', class_='email element')\n",
        "        if email_tag:\n",
        "            script_tag = email_tag.find_next('a').get('href')\n",
        "            if 'cdn-cgi/l/email-protection' in script_tag:\n",
        "                encoded_email = script_tag.split('#')[1]\n",
        "                r = int(encoded_email[:2], 16)\n",
        "                email = ''.join([chr(int(encoded_email[i:i+2], 16) ^ r) for i in range(2, len(encoded_email), 2)])\n",
        "            else:\n",
        "                email = script_tag.replace('mailto:', '')\n",
        "\n",
        "        # Extract services offered by the hotel\n",
        "        services = []\n",
        "        service_wrappers = soup.find_all('div', class_='service_wrap')\n",
        "        for service_wrapper in service_wrappers:\n",
        "            service_text = service_wrapper.get_text(separator=\" \", strip=True)\n",
        "            if service_text:\n",
        "                services.append(service_text)\n",
        "\n",
        "        # Return the extracted information as a dictionary\n",
        "        return {\n",
        "            \"description\": description,\n",
        "            \"latitude\": latitude,\n",
        "            \"longitude\": longitude,\n",
        "            \"address\": address,\n",
        "            \"phone\": phone,\n",
        "            \"email\": email,\n",
        "            \"services\": services\n",
        "        }\n",
        "    else:\n",
        "        # Print error message if the request failed\n",
        "        print(f\"Failed to access the page. Status code: {response.status_code}\")\n",
        "\n",
        "def summarize_description(description):\n",
        "    # Initialize the T5 model and tokenizer for text summarization\n",
        "    model_name = \"t5-small\"\n",
        "    tokenizer = T5Tokenizer.from_pretrained(model_name, legacy=False)\n",
        "    model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "    # Prepare the input text for summarization\n",
        "    input_text = f\"summarize: {description}\"\n",
        "    encoding = tokenizer.encode_plus(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "    # Generate the summary using beam search\n",
        "    generated_ids = model.generate(encoding['input_ids'], num_beams=4, max_length=150, early_stopping=True)\n",
        "    # Decode the generated summary\n",
        "    summarized_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "    return summarized_text\n",
        "\n",
        "def get_hotel_info(hotel_name, city):\n",
        "    # Get the most frequent hotel website\n",
        "    website = get_most_frequent_website(hotel_name, city)\n",
        "    if website:\n",
        "        # Fetch the hotel information from the website\n",
        "        info = fetch_hotel_info(website)\n",
        "        if info and info[\"description\"]:\n",
        "            # Summarize the hotel description\n",
        "            info[\"summary\"] = summarize_description(info[\"description\"])\n",
        "            info['homepage'] = website\n",
        "        return info\n",
        "    else:\n",
        "        # Print error message if the website is not found\n",
        "        print(\"Hotel website not found.\")\n",
        "        return None\n",
        "\n",
        "# Fetch the hotel information\n",
        "info = get_hotel_info(HOTEL_NAME, CITY)\n",
        "pprint.pprint(info)\n",
        "\n",
        "hotel_homepage = info['homepage']\n",
        "if not hotel_homepage.endswith('/'):\n",
        "    hotel_homepage = hotel_homepage + '/'\n",
        "\n",
        "def download_image(url, local_filename):\n",
        "    # Send the GET request to the URL\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
        "    }\n",
        "    response = requests.get(url, headers=headers)\n",
        "\n",
        "    # Check if the request was successful\n",
        "    if response.status_code == 200:\n",
        "        # Open a local file in binary write mode\n",
        "        with open(\"input/\" + local_filename, 'wb') as f:\n",
        "            # Write the response content to the file\n",
        "            f.write(response.content)\n",
        "        print(f\"Image downloaded and saved as {local_filename}\")\n",
        "    else:\n",
        "        # Print error message if the download failed\n",
        "        print(f\"Failed to download the image. HTTP status code: {response.status_code}\")\n",
        "\n",
        "def extract_image_links(url):\n",
        "    # Send a GET request to the specified URL\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
        "    }\n",
        "    response = requests.get(url, headers=headers)\n",
        "\n",
        "    # Ensure the request was successful\n",
        "    if response.status_code != 200:\n",
        "        raise Exception(f\"Failed to load page: {url} Status code: {response.status_code}\")\n",
        "\n",
        "    # Parse the HTML content of the page\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Find all <img> tags\n",
        "    image_links = []\n",
        "\n",
        "    for img in soup.find_all('img'):\n",
        "        src = img.get('src')\n",
        "        if src and not '.svg' in src:\n",
        "            image_links.append(add_https_prefix(src))\n",
        "\n",
        "    # Extract all links from <a> tags that might be images\n",
        "    for a in soup.find_all('a'):\n",
        "        for attr in a.attrs.keys():\n",
        "            if isinstance(a[attr], str) and (a[attr].endswith('.jpg') or a[attr].endswith('.jpeg') or a[attr].endswith('.png')):\n",
        "                image_links.append(add_https_prefix(a[attr]))\n",
        "\n",
        "    # Extract all links from <div> tags that might be images\n",
        "    for a in soup.find_all('div'):\n",
        "        for attr in a.attrs.keys():\n",
        "            if isinstance(a[attr], str) and (a[attr].endswith('.jpg') or a[attr].endswith('.jpeg') or a[attr].endswith('.png')):\n",
        "                image_links.append(add_https_prefix(a[attr]))\n",
        "\n",
        "    # Extract all links from <source> tags that might be images\n",
        "    for a in soup.find_all('source'):\n",
        "        for attr in a.attrs.keys():\n",
        "            if isinstance(a[attr], str) and (a[attr].endswith('.jpg') or a[attr].endswith('.jpeg') or a[attr].endswith('.png')):\n",
        "                image_links.append(add_https_prefix(a[attr]))\n",
        "\n",
        "    # Extract all links from <link> tags that might be images\n",
        "    for a in soup.find_all('link'):\n",
        "        for attr in a.attrs.keys():\n",
        "            if isinstance(a[attr], str) and (a[attr].endswith('.jpg') or a[attr].endswith('.jpeg') or a[attr].endswith('.png')):\n",
        "                image_links.append(add_https_prefix(a[attr]))\n",
        "\n",
        "    # Return unique image links\n",
        "    return list(set(image_links))\n",
        "\n",
        "def add_https_prefix(url):\n",
        "    # Add HTTPS prefix to the URL if missing\n",
        "    if url.startswith(\"//\"):\n",
        "        return \"https:\" + url\n",
        "    elif url.startswith(\"/\"):\n",
        "        return hotel_homepage + url[1:]\n",
        "    return url\n",
        "\n",
        "def is_valid_url(url):\n",
        "    # Check if the URL is valid and starts with http or https\n",
        "    return url.startswith(('http://', 'https://'))\n",
        "\n",
        "def is_internal_link(url, base_url):\n",
        "    # Check if the URL belongs to the same domain\n",
        "    return urlparse(url).netloc == urlparse(base_url).netloc\n",
        "\n",
        "def contains_language_code(url):\n",
        "    # Get the list of all ISO 639-1 language codes\n",
        "    language_codes = [lang.alpha_2 for lang in pycountry.languages if hasattr(lang, 'alpha_2')]\n",
        "    # Check if the URL contains any of the language codes\n",
        "    for code in language_codes:\n",
        "        if f'/{code}/' in url or f'-{code}' in url:\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "def extract_other_pages_url(page_url, home_page_url, links):\n",
        "    # Define the headers for the HTTP request\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
        "    }\n",
        "    # Send the HTTP GET request to the page URL\n",
        "    response = requests.get(page_url, headers=headers)\n",
        "\n",
        "    # Ensure the request was successful\n",
        "    if response.status_code != 200:\n",
        "        raise Exception(f\"Failed to load page: {page_url} Status code: {response.status_code}\")\n",
        "    # Parse the HTML content of the response\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Add the current page URL to the links list\n",
        "    links.append(page_url)\n",
        "\n",
        "    internal_links = []\n",
        "\n",
        "    # Extract all <a> tags with the href attribute\n",
        "    for a_tag in soup.find_all('a', href=True):\n",
        "        href = a_tag['href']\n",
        "\n",
        "        if 'email-protection' in href or '#respond' in href or \".jpg\" in href or \".jpeg\" in href or \".svg\" in href or \".gif\" in href:\n",
        "            continue\n",
        "        # Convert relative URLs to absolute URLs\n",
        "        full_url = urljoin(page_url, href)\n",
        "        if is_valid_url(full_url) and is_internal_link(full_url, home_page_url) and not contains_language_code(full_url):\n",
        "            internal_links.append(full_url)\n",
        "\n",
        "    # Recursively extract internal links from the page\n",
        "    for internal_link in internal_links:\n",
        "        if internal_link not in links:\n",
        "            extract_other_pages_url(internal_link, home_page_url, links)\n",
        "\n",
        "def get_all_website_images(home_page):\n",
        "    # Initialize lists to store page links and image links\n",
        "    pages_links = []\n",
        "    all_image_links = []\n",
        "    # Extract all internal page URLs from the website\n",
        "    extract_other_pages_url(home_page, home_page, pages_links)\n",
        "\n",
        "    pprint.pprint(pages_links)\n",
        "\n",
        "    # Extract image links from each internal page\n",
        "    for page_link in pages_links:\n",
        "        image_links = extract_image_links(page_link)\n",
        "        all_image_links.extend(image_links)\n",
        "\n",
        "    # Remove duplicate image links\n",
        "    all_image_links = list(set(all_image_links))\n",
        "\n",
        "    # Download each image and save it locally\n",
        "    counter = 0\n",
        "    for image_link in all_image_links:\n",
        "        url = add_https_prefix(image_link)\n",
        "        download_image(url, \"image_\" + str(counter) + \".\" + url.split('.')[-1])\n",
        "        counter += 1\n",
        "\n",
        "get_all_website_images(hotel_homepage)\n",
        "\n",
        "def list_files_in_directory(directory_path):\n",
        "    try:\n",
        "        # List all files and directories in the specified path\n",
        "        files_and_dirs = os.listdir(directory_path)\n",
        "\n",
        "        # Filter the list to only include files\n",
        "        files = [os.path.join(directory_path, f).replace('\\\\', '/') for f in files_and_dirs if os.path.isfile(os.path.join(directory_path, f))]\n",
        "\n",
        "        return files\n",
        "    except Exception as e:\n",
        "        # Print error message if listing files failed\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        return []\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, dataframe, transform=None):\n",
        "        self.dataframe = dataframe\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load the image from the specified file path\n",
        "        img_path = self.dataframe.iloc[idx, 0]\n",
        "        with warnings.catch_warnings():\n",
        "            warnings.simplefilter(\"ignore\", UserWarning)\n",
        "            image = Image.open(img_path).convert('RGB')\n",
        "        # Get the label for the image\n",
        "        label = self.dataframe.iloc[idx, 1]\n",
        "        # Apply the transformations to the image\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "# Define the transformations to be applied to each image\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize to 224x224\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class ImageClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ImageClassifier, self).__init__()\n",
        "        # Define the layers of the CNN\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 53 * 53, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 8)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Define the forward pass\n",
        "        x = self.pool(functional.relu(self.conv1(x)))\n",
        "        x = self.pool(functional.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 53 * 53)\n",
        "        x = functional.relu(self.fc1(x))\n",
        "        x = functional.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "model = ImageClassifier().to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Load the pre-trained model weights\n",
        "model.load_state_dict(torch.load(MODEL_PATH, map_location=torch.device('cpu'), weights_only=False))\n",
        "\n",
        "def build_dataframe_from_input(input_path):\n",
        "    # List all files in the input directory\n",
        "    inputs = list_files_in_directory(input_path)\n",
        "    # Create a DataFrame with file paths and target labels\n",
        "    inputs_df = pd.DataFrame({'filepath': inputs, 'target': [-1] * len(inputs)})\n",
        "    return inputs_df\n",
        "\n",
        "def build_dataloader_from_dataframe(df):\n",
        "    # Create a Dataset and DataLoader from the DataFrame\n",
        "    ds = ImageDataset(df, transform)\n",
        "    dl = DataLoader(ds, batch_size=32, shuffle=False)\n",
        "    return dl\n",
        "\n",
        "test_df = build_dataframe_from_input(INPUT_PATH)\n",
        "test_dl = build_dataloader_from_dataframe(test_df)\n",
        "\n",
        "predictions = []\n",
        "with torch.no_grad():\n",
        "    for data in test_dl:\n",
        "        images, labels = data\n",
        "        # Transfer data to GPU\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        # Perform a forward pass through the model\n",
        "        outputs = model(images)\n",
        "        # Get the predicted labels\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        predictions = predictions + predicted.cpu().numpy().tolist()\n",
        "\n",
        "# Create a copy of the DataFrame and add predictions\n",
        "result_df = test_df.copy().drop(['target'], axis=1)\n",
        "result_df['prediction'] = predictions\n",
        "\n",
        "# Print and copy the files for each prediction category\n",
        "for num in num_to_label.keys():\n",
        "    print(f'Elements of type {num_to_label[num]}:')\n",
        "    current_prediction_files = result_df[result_df['prediction'] == num]['filepath'].tolist()\n",
        "    pprint.pprint(current_prediction_files)\n",
        "    for file in current_prediction_files:\n",
        "        shutil.copyfile(file, f'output/{num_to_label[num]}/{file.split(\"/\")[-1]}')"
      ]
    }
  ]
}